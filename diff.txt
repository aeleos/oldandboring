diff -r kernel/arch/x86_64/boot.asm ../Downloads/blog_os/src/arch/x86_64/boot.asm
7,8c7,8
<     mov esp, stack_top ; Update esp with the pointer to our stack at the bottom
<     mov edi, ebx
---
>     mov esp, stack_top
>     mov edi, ebx       ; move Multiboot info pointer to edi
10,12c10,12
<     call check_multiboot ; Check for mutliboot magic number
<     call check_cpuid ; Check for required CPU features
<     call check_long_mode ; Check that long mode is enabled
---
>     call check_multiboot
>     call check_cpuid
>     call check_long_mode
14,15c14,15
<     call set_up_page_tables ; Map the P2 page tables
<     call enable_paging     ; enable paging
---
>     call set_up_page_tables
>     call enable_paging
16a17
>     ; load the 64-bit GDT
20,22d20
<     ; print `OK` to screen
<     ; mov dword [0xb8000], 0x2f4b2f4f
<     ; real_print 'Printing without a db in NASM!',0Dh,0Ah    ; Print out a little message!
23a22,23
>     ; print `OK` to screen
>     mov dword [0xb8000], 0x2f4b2f4f
26,83d25
< 
< set_up_page_tables:
<   ; map P4 table recursively
<   mov eax, p4_table
<   or eax, 0b11 ; present + writable
<   mov [p4_table + 511 * 8], eax
< 
< 
<   ; map first P4 entry to P3 table
<   mov eax, p3_table
<   or eax, 0b11 ; present + writable
<   mov [p4_table], eax
< 
<   ; map first P3 entry to P2 table
<   mov eax, p2_table
<   or eax, 0b11 ; present + writable
<   mov [p3_table], eax
< 
<   ; TODO map each P2 entry to a huge 2MiB page
<   mov ecx, 0         ; counter variable
< 
<   .map_p2_table:
<   ; map ecx-th P2 entry to a huge page that starts at address 2MiB*ecx
<   mov eax, 0x200000  ; 2MiB
<   mul ecx            ; start address of ecx-th page
<   or eax, 0b10000011 ; present + writable + huge
<   mov [p2_table + ecx * 8], eax ; map ecx-th entry
< 
<   inc ecx            ; increase counter
<   cmp ecx, 512       ; if counter == 512, the whole P2 table is mapped
<   jne .map_p2_table  ; else map the next entry
< 
<   ret
< 
< enable_paging:
<     ; load P4 to cr3 register (cpu uses this to access the P4 table)
<     mov eax, p4_table
<     mov cr3, eax
< 
<     ; enable PAE-flag in cr4 (Physical Address Extension)
<     mov eax, cr4
<     or eax, 1 << 5
<     mov cr4, eax
< 
<     ; set the long mode bit in the EFER MSR (model specific register)
<     mov ecx, 0xC0000080
<     rdmsr
<     or eax, 1 << 8
<     wrmsr
< 
<     ; enable paging in the cr0 register
<     mov eax, cr0
<     or eax, 1 << 31
<     mov cr0, eax
< 
<     ret
< 
< 
85,91c27,29
<     ; The multiboot specification says that this number must be loaded
<     ; into eax before the kernel is loaded. We should check to make sure
<     cmp eax, 0x36d76289 ; Check the value
<     jne .no_multiboot ; If not equal, jump to .no_multiboot
<     ret ; Otherwise, return
< 
< 
---
>     cmp eax, 0x36d76289
>     jne .no_multiboot
>     ret
93,94c31,32
<     mov al, "0" ; Have the error function print 0 if multiboot didn't work
<     jmp error ; Jump to error function
---
>     mov al, "0"
>     jmp error
148a87,141
> set_up_page_tables:
>     ; map P4 table recursively
>     mov eax, p4_table
>     or eax, 0b11 ; present + writable
>     mov [p4_table + 511 * 8], eax
> 
>     ; map first P4 entry to P3 table
>     mov eax, p3_table
>     or eax, 0b11 ; present + writable
>     mov [p4_table], eax
> 
>     ; map first P3 entry to P2 table
>     mov eax, p2_table
>     or eax, 0b11 ; present + writable
>     mov [p3_table], eax
> 
>     ; map each P2 entry to a huge 2MiB page
>     mov ecx, 0         ; counter variable
> 
> .map_p2_table:
>     ; map ecx-th P2 entry to a huge page that starts at address 2MiB*ecx
>     mov eax, 0x200000  ; 2MiB
>     mul ecx            ; start address of ecx-th page
>     or eax, 0b10000011 ; present + writable + huge
>     mov [p2_table + ecx * 8], eax ; map ecx-th entry
> 
>     inc ecx            ; increase counter
>     cmp ecx, 512       ; if counter == 512, the whole P2 table is mapped
>     jne .map_p2_table  ; else map the next entry
> 
>     ret
> 
> enable_paging:
>     ; load P4 to cr3 register (cpu uses this to access the P4 table)
>     mov eax, p4_table
>     mov cr3, eax
> 
>     ; enable PAE-flag in cr4 (Physical Address Extension)
>     mov eax, cr4
>     or eax, 1 << 5
>     mov cr4, eax
> 
>     ; set the long mode bit in the EFER MSR (model specific register)
>     mov ecx, 0xC0000080
>     rdmsr
>     or eax, 1 << 8
>     wrmsr
> 
>     ; enable paging in the cr0 register
>     mov eax, cr0
>     or eax, 1 << 31
>     mov cr0, eax
> 
>     ret
> 
152,154c145,146
<     ; 4f is the color of the text and background
<     mov dword [0xb8000], 0x4f524f45 ; 52 is E, 45 is R
<     mov dword [0xb8004], 0x4f3a4f52 ; 3a is
---
>     mov dword [0xb8000], 0x4f524f45
>     mov dword [0xb8004], 0x4f3a4f52
174c166
< .code: equ $ - gdt64 ; automatically calculate offset
---
> .code: equ $ - gdt64 ; new
diff -r kernel/arch/x86_64/grub.cfg ../Downloads/blog_os/src/arch/x86_64/grub.cfg
0a1,2
> set timeout=0
> set default=0
2,4c4,6
< menuentry "BoringOS" {
< 	multiboot2 /boot/kernel.bin
< 	boot
---
> menuentry "my os" {
>     multiboot2 /boot/kernel.bin
>     boot
Only in ../Downloads/blog_os/src/arch/x86_64: long_mode_init.asm
Only in kernel/arch/x86_64: long_mode_start.asm
Only in kernel/arch/x86_64: pushaq.macro
Only in kernel/: .gitignore
diff -r kernel/lib.rs ../Downloads/blog_os/src/lib.rs
14d13
< 
19,20d17
< 
< 
22c19
< pub extern "C" fn rust_main(multiboot_information_address: usize) {
---
> pub extern fn rust_main(multiboot_information_address: usize) {
25,26d21
<     // ATTENTION: we have a very small stack and no guard page
< 
28,43c23
<     println!("Hello {} world", "rust");
< 
<     let boot_info = unsafe { multiboot2::load(multiboot_information_address) };
< 
<     let memory_map_tag = boot_info.memory_map_tag().expect("Memory map tag required");
< 
<     let elf_sections_tag = boot_info.elf_sections_tag().expect(
<         "Elf-sections tag required",
<     );
< 
<     let kernel_start = elf_sections_tag.sections().map(|s| s.addr).min().unwrap();
<     let kernel_end = elf_sections_tag
<         .sections()
<         .map(|s| s.addr + s.size)
<         .max()
<         .unwrap();
---
>     println!("Hello World{}", "!");
44a25,34
>     let boot_info = unsafe{ multiboot2::load(multiboot_information_address) };
>     let memory_map_tag = boot_info.memory_map_tag()
>         .expect("Memory map tag required");
>     let elf_sections_tag = boot_info.elf_sections_tag()
>         .expect("Elf sections tag required");
> 
>     let kernel_start = elf_sections_tag.sections().map(|s| s.addr)
>         .min().unwrap();
>     let kernel_end = elf_sections_tag.sections().map(|s| s.addr + s.size)
>         .max().unwrap();
48,57c38,41
<     println!(
<         "kernel start: 0x{:x}, kernel end: 0x{:x}",
<         kernel_start,
<         kernel_end
<     );
<     println!(
<         "multiboot start: 0x{:x}, multiboot end: 0x{:x}",
<         multiboot_start,
<         multiboot_end
<     );
---
>     println!("kernel start: 0x{:x}, kernel end: 0x{:x}",
>         kernel_start, kernel_end);
>     println!("multiboot start: 0x{:x}, multiboot end: 0x{:x}",
>         multiboot_start, multiboot_end);
60,65c44,45
<         kernel_start as usize,
<         kernel_end as usize,
<         multiboot_start,
<         multiboot_end,
<         memory_map_tag.memory_areas(),
<     );
---
>         kernel_start as usize, kernel_end as usize, multiboot_start,
>         multiboot_end, memory_map_tag.memory_areas());
66a47,48
>     enable_nxe_bit();
>     enable_write_protect_bit();
68c50
<     println!("I can't believe it worked");
---
>     println!("It did not crash!");
70c52,53
<     // memory::test_paging(&mut frame_allocator);
---
>     loop {}
> }
72,74c55,56
<     // println!("{:?}", frame_allocator.allocate_frame());
<     // println!("{:?}", frame_allocator.allocate_frame());
<     // println!("{:?}", frame_allocator.allocate_frame());
---
> fn enable_nxe_bit() {
>     use x86_64::registers::msr::{IA32_EFER, rdmsr, wrmsr};
75a58,63
>     let nxe_bit = 1 << 11;
>     unsafe {
>         let efer = rdmsr(IA32_EFER);
>         wrmsr(IA32_EFER, efer | nxe_bit);
>     }
> }
77c65,66
<     loop {}
---
> fn enable_write_protect_bit() {
>     use x86_64::registers::control_regs::{cr0, cr0_write, Cr0};
78a68
>     unsafe { cr0_write(cr0() | Cr0::WRITE_PROTECT) };
81,82c71
< #[lang = "eh_personality"]
< extern "C" fn eh_personality() {}
---
> #[lang = "eh_personality"] extern fn eh_personality() {}
86c75
< pub extern "C" fn panic_fmt(fmt: core::fmt::Arguments, file: &'static str, line: u32) -> ! {
---
> pub extern fn panic_fmt(fmt: core::fmt::Arguments, file: &'static str, line: u32) -> ! {
88,89c77,78
<     println!("  {}", fmt);
<     loop {}
---
>     println!("    {}", fmt);
>     loop{}
diff -r kernel/memory/area_frame_allocator.rs ../Downloads/blog_os/src/memory/area_frame_allocator.rs
19c19
<             let frame = Frame { number: self.next_free_frame.number };
---
>             let frame = Frame{ number: self.next_free_frame.number };
32c32,34
<                 self.next_free_frame = Frame { number: self.kernel_end.number + 1 };
---
>                 self.next_free_frame = Frame {
>                     number: self.kernel_end.number + 1
>                 };
35c37,39
<                 self.next_free_frame = Frame { number: self.multiboot_end.number + 1 };
---
>                 self.next_free_frame = Frame {
>                     number: self.multiboot_end.number + 1
>                 };
54,60c58,61
<     pub fn new(
<         kernel_start: usize,
<         kernel_end: usize,
<         multiboot_start: usize,
<         multiboot_end: usize,
<         memory_areas: MemoryAreaIter,
<     ) -> AreaFrameAllocator {
---
>     pub fn new(kernel_start: usize, kernel_end: usize,
>         multiboot_start: usize, multiboot_end: usize,
>         memory_areas: MemoryAreaIter) -> AreaFrameAllocator
>     {
75,81c76,79
<         self.current_area = self.areas
<             .clone()
<             .filter(|area| {
<                 let address = area.base_addr + area.length - 1;
<                 Frame::containing_address(address as usize) >= self.next_free_frame
<             })
<             .min_by_key(|area| area.base_addr);
---
>         self.current_area = self.areas.clone().filter(|area| {
>             let address = area.base_addr + area.length - 1;
>             Frame::containing_address(address as usize) >= self.next_free_frame
>         }).min_by_key(|area| area.base_addr);
diff -r kernel/memory/mod.rs ../Downloads/blog_os/src/memory/mod.rs
1a2
> pub use self::paging::remap_the_kernel;
3d3
< pub use self::paging::{test_paging, remap_the_kernel};
17c17
<         Frame { number: address / PAGE_SIZE }
---
>         Frame{ number: address / PAGE_SIZE }
53c53
< }
---
>  }
diff -r kernel/memory/paging/entry.rs ../Downloads/blog_os/src/memory/paging/entry.rs
1a2
> use multiboot2::ElfSection;
21c22
<                 self.0 as usize & 0x000fffff_fffff000,
---
>                 self.0 as usize & 0x000fffff_fffff000
35,45c36,69
<     pub struct EntryFlags: u64 {
<         const PRESENT =         1 << 0;
<         const WRITABLE =        1 << 1;
<         const USER_ACCESSIBLE = 1 << 2;
<         const WRITE_THROUGH =   1 << 3;
<         const NO_CACHE =        1 << 4;
<         const ACCESSED =        1 << 5;
<         const DIRTY =           1 << 6;
<         const HUGE_PAGE =       1 << 7;
<         const GLOBAL =          1 << 8;
<         const NO_EXECUTE =      1 << 63;
---
>     pub flags EntryFlags: u64 {
>         const PRESENT =         1 << 0,
>         const WRITABLE =        1 << 1,
>         const USER_ACCESSIBLE = 1 << 2,
>         const WRITE_THROUGH =   1 << 3,
>         const NO_CACHE =        1 << 4,
>         const ACCESSED =        1 << 5,
>         const DIRTY =           1 << 6,
>         const HUGE_PAGE =       1 << 7,
>         const GLOBAL =          1 << 8,
>         const NO_EXECUTE =      1 << 63,
>     }
> }
> 
> 
> impl EntryFlags {
>     pub fn from_elf_section_flags(section: &ElfSection) -> EntryFlags {
>         use multiboot2::{ELF_SECTION_ALLOCATED, ELF_SECTION_WRITABLE,
>             ELF_SECTION_EXECUTABLE};
> 
>         let mut flags = EntryFlags::empty();
> 
>         if section.flags().contains(ELF_SECTION_ALLOCATED) {
>             // section is loaded to memory
>             flags = flags | PRESENT;
>         }
>         if section.flags().contains(ELF_SECTION_WRITABLE) {
>             flags = flags | WRITABLE;
>         }
>         if !section.flags().contains(ELF_SECTION_EXECUTABLE) {
>             flags = flags | NO_EXECUTE;
>         }
> 
>         flags
diff -r kernel/memory/paging/mapper.rs ../Downloads/blog_os/src/memory/paging/mapper.rs
13c13,15
<         Mapper { p4: Unique::new(table::P4) }
---
>         Mapper {
>             p4: Unique::new(table::P4),
>         }
24,26d25
<     // Translates a virtual address to the corresponding
<     // phyiscal address
<     // Returns `None` if the address is not mapped
41a41
>                         // address must be 1GiB aligned
44,45c44,45
<                             number: start_frame.number + page.p2_index() * ENTRY_COUNT +
<                                 page.p1_index(),
---
>                             number: start_frame.number + page.p2_index() *
>                                     ENTRY_COUNT + page.p1_index(),
49d48
< 
54a54
>                             // address must be 2MiB aligned
56c56,58
<                             return Some(Frame { number: start_frame.number + page.p1_index() });
---
>                             return Some(Frame {
>                                 number: start_frame.number + page.p1_index()
>                             });
60d61
< 
66,68c67,69
<             .and_then(|p2| p2.next_table(page.p2_index()))
<             .and_then(|p1| p1[page.p1_index()].pointed_frame())
<             .or_else(huge_page)
---
>         .and_then(|p2| p2.next_table(page.p2_index()))
>         .and_then(|p1| p1[page.p1_index()].pointed_frame())
>         .or_else(huge_page)
71,78c72,74
< 
<     // Maps the page to the frame with the provided flags.
<     // The `PRESRNT` flag is added by default.
<     // It needs a `FrameAllocator` as it might
<     // Need to create new page tables.
<     pub fn map_to<A>(&mut self, page: Page, frame: Frame, flags: EntryFlags, allocator: &mut A)
<     where
<         A: FrameAllocator,
---
>     pub fn map_to<A>(&mut self, page: Page, frame: Frame, flags: EntryFlags,
>                     allocator: &mut A)
>         where A: FrameAllocator
80c76,77
<         let mut p3 = self.p4_mut().next_table_create(page.p4_index(), allocator);
---
>         let p4 = self.p4_mut();
>         let mut p3 = p4.next_table_create(page.p4_index(), allocator);
82a80
> 
87,88d84
<     // Maps the page to some free frame with the provided flags.
<     // The free frame is allocated from the given `FrameAllocator`
90,91c86
<     where
<         A: FrameAllocator,
---
>         where A: FrameAllocator
95d89
< 
98,100d91
<     // Identity map the given frame with the provided flags
<     // The `FrameAllocator` is used to create new page
<     // tables if needed
102,103c93
<     where
<         A: FrameAllocator,
---
>         where A: FrameAllocator
109,110d98
<     // Unmaps the given page and adds all freed frames
<     // to the given `FrameAllocator`
112,113c100
<     where
<         A: FrameAllocator,
---
>         where A: FrameAllocator
114a102,104
>         use x86_64::instructions::tlb;
>         use x86_64::VirtualAddress;
> 
118,121c108,111
<             .next_table_mut(page.p4_index())
<             .and_then(|p3| p3.next_table_mut(page.p3_index()))
<             .and_then(|p2| p2.next_table_mut(page.p2_index()))
<             .expect("mapping code does not support huge pages");
---
>                     .next_table_mut(page.p4_index())
>                     .and_then(|p3| p3.next_table_mut(page.p3_index()))
>                     .and_then(|p2| p2.next_table_mut(page.p2_index()))
>                     .expect("mapping code does not support huge pages");
124,126d113
< 
<         use x86_64::instructions::tlb;
<         use x86_64::VirtualAddress;
128,129d114
< 
< 
131c116
<         // allocator.deallocate_frame(frame);
---
>         //allocator.deallocate_frame(frame);
diff -r kernel/memory/paging/mod.rs ../Downloads/blog_os/src/memory/paging/mod.rs
3,9d2
< 
< use self::table::{Table, Level4};
< use self::temporary_page::TemporaryPage;
< 
< use memory::{PAGE_SIZE, Frame, FrameAllocator};
< 
< use core::ptr::Unique;
11c4,5
< 
---
> use core::ptr::Unique;
> use memory::{PAGE_SIZE, Frame, FrameAllocator};
13c7,8
< 
---
> use self::table::{Table, Level4};
> use self::temporary_page::TemporaryPage;
27c22
<     number: usize,
---
>    number: usize,
32,36c27,29
<         assert!(
<             address < 0x0000_8000_0000_0000 || address >= 0xffff_8000_0000_0000,
<             "invalid address: 0x{:x}",
<             address
<         );
---
>         assert!(address < 0x0000_8000_0000_0000 ||
>             address >= 0xffff_8000_0000_0000,
>             "invalid address: 0x{:x}", address);
40c33
<     pub fn start_address(&self) -> usize {
---
>     fn start_address(&self) -> usize {
47d39
< 
51d42
< 
55d45
< 
81c71,73
<         ActivePageTable { mapper: Mapper::new() }
---
>         ActivePageTable {
>             mapper: Mapper::new(),
>         }
84,90c76,80
<     pub fn with<F>(
<         &mut self,
<         table: &mut InactivePageTable,
<         temporary_page: &mut temporary_page::TemporaryPage,
<         f: F,
<     ) where
<         F: FnOnce(&mut Mapper),
---
>     pub fn with<F>(&mut self,
>                    table: &mut InactivePageTable,
>                    temporary_page: &mut temporary_page::TemporaryPage, // new
>                    f: F)
>         where F: FnOnce(&mut Mapper)
96c86,87
<             let backup = Frame::containing_address(unsafe { control_regs::cr3().0 } as usize);
---
>             let backup = Frame::containing_address(
>                 control_regs::cr3().0 as usize);
98c89
<             // map the temporary_page to current p4 table
---
>             // map temporary_page to current p4 table
101c92
<             //overwrite recursive mapping
---
>             // overwrite recursive mapping
105c96
<             //execute f in the new context
---
>             // execute f in the new context
107a99
>             // restore recursive mapping to original p4 table
113a106,121
> 
>     pub fn switch(&mut self, new_table: InactivePageTable) -> InactivePageTable {
>         use x86_64::PhysicalAddress;
>         use x86_64::registers::control_regs;
> 
>         let old_table = InactivePageTable {
>             p4_frame: Frame::containing_address(
>                 control_regs::cr3().0 as usize
>             ),
>         };
>         unsafe {
>             control_regs::cr3_write(PhysicalAddress(
>                 new_table.p4_frame.start_address() as u64));
>         }
>         old_table
>     }
121,127c129,132
<     pub fn new(
<         frame: Frame,
<         active_table: &mut ActivePageTable,
<         temporary_page: &mut TemporaryPage,
<     ) -> InactivePageTable {
<         // extra scope to ensure table variable is dropped
<         // along with its exclusive borrow of temporary_page
---
>     pub fn new(frame: Frame,
>                active_table: &mut ActivePageTable,
>                temporary_page: &mut TemporaryPage)
>                -> InactivePageTable {
129c134,135
<             let table = temporary_page.map_table_frame(frame.clone(), active_table);
---
>             let table = temporary_page.map_table_frame(frame.clone(),
>                 active_table);
132c138
<             // and set up recursive mapping for it
---
>             // set up recursive mapping for the table
141,169d146
< pub fn test_paging<A>(allocator: &mut A)
< where
<     A: FrameAllocator,
< {
<     let mut page_table = unsafe { ActivePageTable::new() };
< 
<     let addr = 42 * 512 * 512 * 4096;
<     let page = Page::containing_address(addr);
<     let frame = allocator.allocate_frame().expect("no more frames");
<     println!(
<         "None = {:?}, map to {:?}",
<         page_table.translate(addr),
<         frame
<     );
<     page_table.map_to(page, frame, EntryFlags::empty(), allocator);
<     println!("Some = {:?}", page_table.translate(addr));
<     println!("next free frame: {:?}", allocator.allocate_frame());
< 
<     println!("{:#x}", unsafe {
<         *(Page::containing_address(addr).start_address() as *const u64)
<     });
< 
< 
<     page_table.unmap(Page::containing_address(addr), allocator);
<     println!("None = {:?}", page_table.translate(addr));
< 
< }
< 
< 
171,172c148
< where
<     A: FrameAllocator,
---
>     where A: FrameAllocator
174c150,151
<     let mut temporary_page = TemporaryPage::new(Page { number: 0xcafebabe }, allocator);
---
>     let mut temporary_page = TemporaryPage::new(Page { number: 0xcafebabe },
>         allocator);
177d153
< 
184,186c160,161
<         let elf_sections_tag = boot_info.elf_sections_tag().expect(
<             "Memory map tag required",
<         );
---
>         let elf_sections_tag = boot_info.elf_sections_tag()
>             .expect("Memory map tag required");
189,190c164,176
<             for section in elf_sections_tag.sections() {
<                 use self::entry::WRITABLE;
---
>             use self::entry::WRITABLE;
> 
>             if !section.is_allocated() {
>                 // section is not loaded to memory
>                 continue;
>             }
>             assert!(section.start_address() % PAGE_SIZE == 0,
>                     "sections need to be page aligned");
> 
>             println!("mapping section at addr: {:#x}, size: {:#x}",
>                 section.addr, section.size);
> 
>             let flags = EntryFlags::from_elf_section_flags(section);
192,216c178,181
<                 if !section.is_allocated() {
<                     // section is not loaded into memory
<                     continue;
<                 }
< 
<                 assert!(
<                     section.start_address() % PAGE_SIZE == 0,
<                     "sections need to be page aligned"
<                 );
< 
<                 println!(
<                     "mapping section at addr: {:#x}, size: {:#x}",
<                     section.addr,
<                     section.size
<                 );
< 
<                 let flags = WRITABLE;
< 
<                 let start_frame = Frame::containing_address(section.start_address());
< 
<                 let end_frame = Frame::containing_address(section.end_address());
< 
<                 for frame in Frame::range_inclusive(start_frame, end_frame) {
<                     mapper.identity_map(frame, flags, allocator);
<                 }
---
>             let start_frame = Frame::containing_address(section.start_address());
>             let end_frame = Frame::containing_address(section.end_address() - 1);
>             for frame in Frame::range_inclusive(start_frame, end_frame) {
>                 mapper.identity_map(frame, flags, allocator);
218a184,194
> 
>         // identity map the VGA text buffer
>         let vga_buffer_frame = Frame::containing_address(0xb8000);
>         mapper.identity_map(vga_buffer_frame, WRITABLE, allocator);
> 
>         // identity map the multiboot info structure
>         let multiboot_start = Frame::containing_address(boot_info.start_address());
>         let multiboot_end = Frame::containing_address(boot_info.end_address() - 1);
>         for frame in Frame::range_inclusive(multiboot_start, multiboot_end) {
>             mapper.identity_map(frame, PRESENT, allocator);
>         }
219a196,205
> 
>     let old_table = active_table.switch(new_table);
>     println!("NEW TABLE!!!");
> 
>     // turn the old p4 page into a guard page
>     let old_p4_page = Page::containing_address(
>       old_table.p4_frame.start_address()
>     );
>     active_table.unmap(old_p4_page, allocator);
>     println!("guard page at {:#x}", old_p4_page.start_address());
diff -r kernel/memory/paging/table.rs ../Downloads/blog_os/src/memory/paging/table.rs
14,17c14
< impl<L> Table<L>
< where
<     L: TableLevel,
< {
---
> impl<L> Table<L> where L: TableLevel {
25,28c22
< impl<L> Table<L>
< where
<     L: HierarchicalLevel,
< {
---
> impl<L> Table<L> where L: HierarchicalLevel {
40,42c34,35
<         self.next_table_address(index).map(|address| unsafe {
<             &*(address as *const _)
<         })
---
>         self.next_table_address(index)
>             .map(|address| unsafe { &*(address as *const _) })
46,48c39,40
<         self.next_table_address(index).map(|address| unsafe {
<             &mut *(address as *mut _)
<         })
---
>         self.next_table_address(index)
>             .map(|address| unsafe { &mut *(address as *mut _) })
51,57c43,47
<     pub fn next_table_create<A>(
<         &mut self,
<         index: usize,
<         allocator: &mut A,
<     ) -> &mut Table<L::NextLevel>
<     where
<         A: FrameAllocator,
---
>     pub fn next_table_create<A>(&mut self,
>                                 index: usize,
>                                 allocator: &mut A)
>                                 -> &mut Table<L::NextLevel>
>         where A: FrameAllocator
60,63c50,51
<             assert!(
<                 !self.entries[index].flags().contains(HUGE_PAGE),
<                 "mapping code does not support huge pages"
<             );
---
>             assert!(!self.entries[index].flags().contains(HUGE_PAGE),
>                     "mapping code does not support huge pages");
72,75c60
< impl<L> Index<usize> for Table<L>
< where
<     L: TableLevel,
< {
---
> impl<L> Index<usize> for Table<L> where L: TableLevel {
83,86c68
< impl<L> IndexMut<usize> for Table<L>
< where
<     L: TableLevel,
< {
---
> impl<L> IndexMut<usize> for Table<L> where L: TableLevel {
104c86
< pub trait HierarchicalLevel: TableLevel {
---
> trait HierarchicalLevel: TableLevel {
diff -r kernel/memory/paging/temporary_page.rs ../Downloads/blog_os/src/memory/paging/temporary_page.rs
12,13c12
<     where
<         A: FrameAllocator,
---
>         where A: FrameAllocator
21,23c20,24
<     // Maps the temporary page to the given frame in the active table
<     // Returns the start address of the temporary table.
<     pub fn map(&mut self, frame: Frame, active_table: &mut ActivePageTable) -> VirtualAddress {
---
>     /// Maps the temporary page to the given frame in the active table.
>     /// Returns the start address of the temporary page.
>     pub fn map(&mut self, frame: Frame, active_table: &mut ActivePageTable)
>         -> VirtualAddress
>     {
26,30c27,28
<         assert!(
<             active_table.translate_page(self.page).is_none(),
<             "temporary page is already mapped"
<         );
< 
---
>         assert!(active_table.translate_page(self.page).is_none(),
>                 "temporary page is already mapped");
35c33
<     // Unmaps the temporary page in the active table
---
>     /// Unmaps the temporary page in the active table.
40,44c38,43
<     pub fn map_table_frame(
<         &mut self,
<         frame: Frame,
<         active_table: &mut ActivePageTable,
<     ) -> &mut Table<Level1> {
---
>     /// Maps the temporary page to the given page table frame in the active
>     /// table. Returns a reference to the now mapped table.
>     pub fn map_table_frame(&mut self,
>                         frame: Frame,
>                         active_table: &mut ActivePageTable)
>                         -> &mut Table<Level1> {
53,54c52
<     where
<         A: FrameAllocator,
---
>         where A: FrameAllocator
74c72
<             if frame_option.is_some() {
---
>             if frame_option.is_none() {
79c77
<         panic!("Tiny allocator can only hold 3 frames.");
---
>         panic!("Tiny allocator can hold only 3 frames.");
Only in kernel/: vga_buffer
Only in ../Downloads/blog_os/src/: vga_buffer.rs
